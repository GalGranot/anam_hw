{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "---\n",
    "\n",
    "## Homework 1 - Dimensionality Reduction\n",
    "---\n",
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* Questions\n",
    "    * Matrix derivatives refresher\n",
    "    * CCA\n",
    "    * t-SNE Gradient\n",
    "    * PCA\n",
    "    * PCA & Friends (Exam-ish Question) - BONUS\n",
    "* Python Exercise - KPCA Implementation\n",
    "* Python Exercise - PCA and TSNE\n",
    "\n",
    "#### Use as many cells as you need\n",
    "#### אפשר גם לכתוב בעברית, אבל עדיף באנגלית\n",
    "\n",
    "* Code Tasks are denoted with: <img src=\"https://img.icons8.com/color/48/000000/code.png\">\n",
    "* Questions (which you need to answer in a Markdown cell) are denoted with: <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\">\n",
    "\n",
    "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
    "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Student 1| student_1@campus.technion.ac.il| 123456789|\n",
    "|Student 2| student_2@campus.technion.ac.il| 987654321|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: **100** (even with the bonus, the grade will not be above 100).\n",
    "    * Example: if you got 5 points bonus, but you were right in all sections, your grade will still be 100 (and not 105).\n",
    "    * Example: if you got 5 points bonus, and 6 points were deducted for wrong answers, your grade will be 99.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **ANSWERS TO THEORETICAL/MATHEMATICAL QUESTIONS**:\n",
    "    * **Typed - 5 points bonus**: you can type directly in a Markdown cell using Latex (see cheatsheets above), or use Word, <a href=\"https://www.overleaf.com/\">Overleaf</a>, <a href=\"https://www.lyx.org/\">LyX</a>...\n",
    "        * This is a really good practice, we encourage you to practice your math typing skills.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046202_hw1_id1_id2.ipynb`.\n",
    "    * If you answered the questions in a different file you should submit a `.zip` file with the name `ee046202_hw1_id1_id2.zip` with content:\n",
    "        * `ee046202_hw1_id1_id2.ipynb` - the code tasks\n",
    "        * `ee046202_hw1_id1_id2.pdf` - answers to questions.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
    "---\n",
    "* You can choose your working environment:\n",
    "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both allow editing and running Jupyter Notebooks.\n",
    "\n",
    "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046202-unsupervised-learning-data-analysis) to help you get everything installed.\n",
    "* If you need any technical assistance, please go to our Piazza forum (`hw1` folder) and describe your problem (preferably with images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/info.png\" style=\"height:50px;display:inline\"> Tip\n",
    "---\n",
    "If you find it more convenient, you can copy the section to a new cell, and answer the question just right below it. For example:\n",
    "\n",
    "#### Question 0\n",
    "---\n",
    "1. What is the best course in the Technion?\n",
    "2. Why does no one pick Bulbasaur as first pokemon?\n",
    "3. Why is there no superhero named Catman?\n",
    "\n",
    "#### Answers - Q0\n",
    "---\n",
    "#### Q0 - Section 1\n",
    "* Q: What is the best course in the Technion?\n",
    "\n",
    "ANAM!\n",
    "\n",
    "#### Q0 - Section 2\n",
    "* Q: Why does no one pick Bulbasaur as first pokemon?\n",
    "\n",
    "It is really a riddle....\n",
    "\n",
    "#### Q0 - Section 3\n",
    "* Q: Why is there no superhero named Catman?\n",
    "\n",
    "I got nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 1 - Matrix derivatives\n",
    "---\n",
    "####  Vector & Matrix Deriviatives\n",
    "* $\\nabla_x Ax = A^{T}$\n",
    "* $\\nabla_x x^{T} A x = (A + A^{T}) x$ \n",
    "* $\\frac{\\partial}{\\partial A} \\ln |A| = A^{-T}$\n",
    "* $\\frac{\\partial}{\\partial A} Tr[AB] = B^{T}$\n",
    "\n",
    "Using the above, we will use the following:\n",
    "1. $\\nabla_{\\mu} {\\mu}^{T} \\Sigma^{-1} x_i = \\Sigma^{-1} x_i$ \n",
    "2. $\\nabla_{\\mu} {\\mu}^{T} \\Sigma^{-1} \\mu = (\\Sigma^{-1} + {\\Sigma}^{-T}) \\mu$\n",
    "3. $\\frac{\\partial}{\\partial \\Sigma^{-1}} \\ln |\\Sigma^{-1}| = \\Sigma^{T} = \\Sigma$\n",
    "4. $\\frac{\\partial}{\\partial \\Sigma^{-1}} Tr[\\Sigma^{-1} \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu}) (\\overline{x_i} - \\overline{\\mu})^{T}] = \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu}) (\\overline{x_i} - \\overline{\\mu})^{T}$\n",
    "---\n",
    "####  Question time\n",
    "\n",
    "Let $X_1,\\ldots,X_N\\sim \\mathcal{N}(\\mu,\\Sigma)$ i.i.d.,$\\ \\mu\\in \\mathcal{R}^d,\\ \\Sigma\\in \\mathcal{R}^{d\\times d}$ .\n",
    "\n",
    "1. Find the MLE estimator $\\hat{\\mu}_{MLE}$. \n",
    "    * Reminder: finding the MLE estimator is done by comparing the gradient to zero.\n",
    "    * Use the vector and matrix derivatives above. \n",
    "2. Find the MLE estimator $\\hat{\\Sigma}_{MLE}$\n",
    "     * You should use **The Trace Trick** - $\\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu})^{T} \\Sigma^{-1} (\\overline{x_i} - \\overline{\\mu}) = \\sum_{i=1}^n \\textit{Trace}\\big((\\overline{x_i} - \\overline{\\mu})^{T} \\Sigma^{-1} (\\overline{x_i} - \\overline{\\mu})\\big) = \\textit{Trace}\\big(\\Sigma^{-1} \\sum_{i=1}^n (\\overline{x_i} - \\overline{\\mu}) (\\overline{x_i} - \\overline{\\mu})^{T} \\big)$ .\n",
    "     * It may be easier to use $\\frac{\\partial}{\\partial \\Sigma^{-1}}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 2 - CCA\n",
    "---\n",
    "In the lecture, we defined the CCA problem and the matrix $$M=\\Sigma_{xx}^{-1/2}\\Sigma_{xy}\\Sigma_{yy}^{-1/2}\\in \\mathbb{R}^{D_x\\times D_y}$$.\n",
    "\n",
    "Show that the solution of the CCA problem is obtained via principal eigenvectors of $$MM^T\\in \\mathbb{R}^{D_x\\times D_x},\\ M^TM \\in \\mathbb{R}^{D_y\\times D_y}$$.\n",
    "Hint: Use Lagrange multipliers, find singular vectors for $M, M^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 3 - t-SNE Gradient\n",
    "---\n",
    "Recall the objective of t-SNE algorithm: $$ C=KL(P||Q) =  \\sum_{k,l\\neq k}p_{lk}\\log\\frac{p_{lk}}{q_{lk}} $$\n",
    "Calculate the gradient of the objective function with respect to the low-dimensionality mappings $y_i$, i.e. calculate $\\frac{\\partial C}{\\partial y_i}$.\n",
    "\n",
    "Hint:\n",
    "   * Denote $(1 + ||y_i-y_j||^2)^{-1} = E_{ij}^{-1}$.\n",
    "       * Notice that $E_{ij} = E_{ji}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 4 - PCA\n",
    "---\n",
    "1. Assume we have a collection of data with $m$ samples and $n$ features per sample: $S \\in \\mathcal{R}^{m \\times n}$. Assume the data is already centered. Let $C = S^TS$, $C$ is an $n \\times n$ symmetric matrix and thus diagonalizable. Assum that $C$ has eigenvalues $$ \\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n. $$ Moreover, $C$ has an **orthonormal** basis of eigenvectors $$ v_1, v_2, ..., v_n \\:\\: \\text{such that} \\: C{v_i} = \\lambda_i v_i.  $$ In particular, $$ v_i^T C v_i = \\lambda_i v_i^T v_i = \\lambda_i ||v_i||_2^2 = \\lambda_i. $$ **Claim**: $v_1$ represents the direction of the largest variance of $S$. *Prove* the claim by showing the following: if $u=a_1 v_1 + a_2v_2 + ... + a_n v_n$ is a unit vector in $\\mathcal{R}^n$ then $u^T Cu \\leq \\lambda_1$, i.e., $v_1$ is the direction of the largest variance of $S$.\n",
    "\n",
    "2. **Theorem**: Let $X \\in \\mathcal{R}^{d \\times n}$ be the data matrix with $n$ samples and $d$ features, $W = XX^T$ the covariance matrix (note the difference in definition from section 1), $u_1, ..., u_d$ the eigenvectors of $W$ and $\\lambda_1,..., \\lambda_d$ the corresponding eigenvalues. Let $P_{u_i}: \\mathcal{R}^d \\to \\mathcal{R}^d$ be the projection operator onto the subspace $\\mathrm{span}\\{u_i\\}$. Then $$ \\sum_{j=1}^n ||P_{u_i}x_j ||_2^2 = \\lambda_i .$$ **Prove** the theorem.\n",
    "    * Frobenius Norm: $||A|||_F^2 =\\sum_{i=1}^m \\sum_{j=1}^n ||a_{ij}||_2^2 $\n",
    "    * $||A||^2_F = Tr(AA^T)$\n",
    "    * $W = UDU^{-1}=UDU^T$, $D$ is a diagonal matrix with the eigenvalues on the diagonal.\n",
    "    * The projection operator matrix: $P_{u_i} = u_i u_i^T $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 5 - PCA & Friends (Exam-ish Question) - BONUS\n",
    "---\n",
    "1. Canonical Correlation Analysis (CCA): Consider two zero-mean random vectors, $x \\in \\mathbb{R}^{D_x}$ and $y \\in \\mathbb{R}^{D_y}$. We would like to find a one-dimensional representation of $x$ and $y$ that **maximizes the correlation** between them. Let these 1D representations be given by $a^Tx$ and $b^Ty$, respectively, $a \\in \\mathbb{R}^{D_x}, b \\in \\mathbb{R}^{D_y}$.\n",
    "    * This problem can be formulated as the optimization problem $$ \\max_{a,b} \\mathbb{E}\\left[(a^Tx)(b^Ty)\\right] \\text{ s.t. } \\mathbb{E}\\left[(a^Tx)^2\\right]=1, \\mathbb{E}\\left[(b^Ty)^2\\right]=1 $$ Motivate each term and explain its origin.\n",
    "    * Relate the previous expression to PCA.\n",
    "2. Explain in your own words the difference of the statistical and geometric view point. Specifically,\n",
    "    * Discuss the difference in optimization criteria according to these different view points.\n",
    "    * Explain the redundancy in the MSE solution and explain why it is absent in variance max formulation.\n",
    "3. Show that the conventional linear PCA algorithm is recovered as a special case of Kernal PCA. That is, if we choose the linear kernel function given by $k(x,y) = x^Ty$ we get the PCA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Question 6 - Python - KPCA Implementation\n",
    "---\n",
    "1. Implement the KPCA algorithm (do not use scikit-learn's implementation).\n",
    "    * The KPCA function should have the form `KPCA(X, d, kernel)` where:\n",
    "        * $X \\in \\mathbb{R}^{D \\times N}$ - the data matrix with $N$ data samples and $D$ features.\n",
    "        * $d$ is the final dimension of each data point.\n",
    "        * `kernel` is a **function** which receives $x,y$ and returns $k(x,y)$ for these points.\n",
    "            * When applying the KPCA, you can use Python's `lambda` function to write a one-line function.\n",
    "            * If you don't want to use `lambda`, you can use the regular functions with `def func(): ...`, but you will need to change the signature of KPCA to `KPCA(X, d, kernel, *args)`, where `args` is a list of parameters that will be fed into `kernel`.\n",
    "                * For example, for the Gaussian kernel, you will define a function in the form `GaussianKernel(x,y, sigma)`, and then you will call KPCA like that: `KPCA(X,d, GaussianKernel, sigma): ... k = GaussianKernel(x_1, x_2, sigma) ... `\n",
    "            * Read more: <a href=\"http://book.pythontips.com/en/latest/args_and_kwargs.html\">*args</a>, <a href=\"https://www.w3schools.com/python/python_lambda.asp\"> Lambda functions </a>\n",
    "        * For the eigenvectors and eigenvalues, use `np.linalg.eigh`.\n",
    "        * How to make sure your implementation is correct? You can compare with scikit-learn's KPCA (see Tutorial 1).\n",
    "\n",
    "2. Create a dataset of 1000 points using scikit-learn's `sklearn.datasets.make_s_curve` (use the default parameters). Apply PCA (use the one from the tutorial or scikit-learn's) and your KPCA. Use the Gaussian Kernel and a second (nonlinear) kernel of your choice. Explain how you picked $\\sigma$ for the Gaussian Kernel. Plot the original dataset (in **3D**), and its representations in 2D you obtained (4 plots in total: Original, PCA, KPCA (Gaussian), KPCA (Other) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# you can add more if you need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_s_curve\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your Code Here (add as many cells as you need)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Question 7 - Python - Dimensionality reduction methods\n",
    "---\n",
    "In this exercise we are going to compare different dimensionality reduction techniques on the Wine dataset.\n",
    "The wine dataset is a classic and very easy multi-class classification dataset. There are 3 types of wine, 178 examples with 13 features each. Even though it is a very dataset for classification, we will do unsupervised analysis to compare different aspects of dimensionality reduction methods. Let's look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the question\n",
    "# you can add more if you wish (but it is not really needed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine, load_digits\n",
    "\n",
    "X, y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 7.1 - Importance of Feature Scaling\n",
    "---\n",
    "* Perform PCA on the data (`X`) to `n_components=2` and plot it.\n",
    "* Scale the data using `StandardSaler()` to create `X_scaled`, perform PCA on `X_scaled` to `n_components=2` and plot it.\n",
    "* Color the datapoints according to their class: `ax.scatter(X[:,0], X[:,1], c=y)`\n",
    "\n",
    "Note: for all algorithms, use constant seed (`random_state=...`), for example: `PCA(n_component=2, random_state=random_state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 7.1 - Importance of Feature Scaling\n",
    "---\n",
    "- Why is it so important to perform feature scaling before performing dimensionality reduction, espicially for PCA?\n",
    "- Describe the results of Task 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 7.2 - T-SNE\n",
    "---\n",
    "* Perform t-SNE on the scaled dataset to `n_components=2` and plot it.\n",
    "* Find the `perplexity`, that yields the best-looking results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 7.3 - Robustness to Noise & Outliers\n",
    "---\n",
    "In this task we are going to test how robust are t-SNE and PCA to noisy features and outliers. We will have 2 new datasets:\n",
    "1. `X_noisy` - random normal noise ($\\mathcal{N}(0,1)$) is added to the current features after scaling.\n",
    "2. `X_skewed` - 20 random samples (random feature values) are added to the sample (total samples: 178 + 20 = 198)\n",
    "\n",
    "The tasks:\n",
    "* Perform standardization to create `X_noisy_scaled`, `X_skewed_scaled`\n",
    "* Peform PCA and t-SNE to both datasets (4 in total) to `n_components=2` (as before) and plot the results (4 plots, can be in pairs and can be a 2X2 plot, don't forget to put titles).\n",
    "    * You should tune the `perplexity` for t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 7.3 - Robustness to Noise & Outliers\n",
    "---\n",
    "- Explain the results. In your answer, describe the effect of adding noise and outliers, which algorithm is more affected and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
